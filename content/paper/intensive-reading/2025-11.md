## Tokencake

- `KV cache, serve, multi-agent` **Tokencake: A KV-Cache-centric Serving Frameworkfor LLM-based Multi-Agent Applications**. Zhuohang Bian et.al. **Arxiv'2510**, [link](https://arxiv.org/pdf/2510.18586)
  - PKU
  - research problem:
    - multi-agent app serving
      - kv cacge闲置导致bs开不大。ayo,仅仅pipeline去重叠llm和tool call,但是kvcache stalled,没有被利用,这里的kv cache指掉func call的llm的kv cache,多agent这个问题更严重
      - kv cache关键路径被逐出的问题，FCFS会导致重要kv cache被逐出，导致重计算
        - 本质上就是多个分支下，关键分支（执行时间长）由于图的结构被抢占
  - key insight:
    - 可以利用stalled的agent的那部分显存去做别的请求的计算
    - 关键路径上的agent应该预先保留一部分一资源
  - key idea:
    - managing KV Cache resources across both time and space dimensions. 去提升显存利用率。
  - core method:
    - time scheduler
      - 事件驱动的offload,当tool调用时offload,预测tool时间prefetch
      - offline冷启动预测时间 + online动态修正预测时间
      - 根据收益函数判断是否offload,有请求可以在tool的这段时间内完成
      - 优化offload开销，offload大量page block开销大，采用GPU块缓存
      - 解决prefetch时有可用的gpu显存： 根据预测时间逐步回收显存
    - space scheduler
      - 显存分成两部分，一部分单独留给重要的agent
        - 重要agent是动态选择的,根据静态（DAG）+动态（prefer short req）确定重要性
        - 根据agent历史数据和重要性确定显存分配比例
  - TLDR:
    - 定义的非常好，agent-tool + agent-agent interaction
    - 场景非常不错，multi-agent coding, deep research
    - 测量了不同tool的latency distribution,有参考意义
    - 关于请求pipeline的过程没有说明，可能是侧重点的问题把

## KVCOMM

- `kv cache, serve, multi-agent` **KVCOMM\_ Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems**. Hancheng Ye et.al. **Arxiv'2511 & NeurIPS'25**, [link](https://arxiv.org/pdf/2510.12872) [Code](https://github.com/FastMAS/KVCOMM)
  - DukeU & MIT & Nvidia
  - research problem:
    - agnet前缀不同导致难复用的问题
  - key insight:
  - key idea:
  - core method:
  - TLDR:
    - 提到必须模型一样才可以复用

## KVFLOW

- `kv cache, multi-agent, kv-evict` **KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows**. Zaifeng and et.al. **Arxiv'2507**, [link](https://www.kdocs.cn/l/ckUoJAKSukFJ)
  - UCSD & AWS
  - research problem:
    - 传统的逐出策略如LRU没有考虑到agent workflow，导致可重用的kv cache被驱逐
    - 关注的是类似前缀prompt重用,可以想象一个图里,一个agent在头尾都可能出现
  - key insight:
  - key idea:根据workflow确定prefix cache的逐出顺序
  - core method:
    - 树状管理的kv cache缓存(多个提示词也会有相同的前缀,所以弄成树状) with slgang,节点关联下面几轮可能被使用,根据这个值确定驱逐策略
      - 优先驱逐非prompt的kv cache
    - prefetch重叠,比较简单,提前+并行节点优先调度有prefix cache的
  - TLDR:
    - 为什么是树状真的会有这么多llm agent吗?这些fixed前缀真的能很大吗?最多10k这样把,多吗?
    - serve会有这个问题吗,这个只有该agent闲置的时候才会这样做,实验里感觉高并发提升很小
    - 实验 8000/1000/1000 token数量这个是否合理

## Cortex

- `schedule, serving, multi-agent, resource management` **Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving**. Nikos Pagonas et.al. **Arxiv'2510**, [link](https://www.kdocs.cn/l/ceE1xkKCVnjm)
  - Columbia & Google
  - research problem:
  - key insight:每个agent分配单独的engine可以显著提升kv cache利用率
    - 不同agent模块有不同的内存需求
    - 变化的图结构-分支数量和深度,也是随agent特性变化的
  - key idea:
    - 为每个agent分配单独的engine,而不是用一个engine,降低内存使用量,加大BS
  - core method:
    - 早起版本,没怎么说
  - TLDR:
    - 这个挺有意思的,一样的模型还要整几个engine,或许有东西可以做
    - 思考一下多个agent混在一起做decode是不是会有SLO不稳定的问题,所以要分离

## Agent plan caching

- `plan cache reuse, agent, serve` **Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching**. et.al. **Qizheng Zhang**, [link](https://www.kdocs.cn/l/ctRpTQzZOZss)
  - Stanford
  - research problem:React模式下,尽管agent app的输入一样,但由于上下文不同,如外部文档变化,导致多次循环下不能复用cache
  - key insight:生成plan时间占主导,且plan可以被重用
  - key idea:使用关键词匹配历史的plan cache,plan cache是去除了具体信息的模板
  - core method:
  - TLDR:比较偏算法;只适用比较简单的workflow如react;

## Compound llm system

- `agent, uncertainty` **Towards Efficient Compound Large Language Model System Serving in the Wild.** **IWQoS'2024**
  - SJTU
  - workshop
  - optim e2e latency
  - Challenge：DAG的不确定性->topology;exec duration;
  - motivation：“信息的生成（llm planer）”本身就是一个关键的、需要被优先保障的计算过程 。优先生成DAG方便调度后续步骤，提高资源利用率。
  - Solution：**PS-TCS** (Priority-based Scheduling with Topological Complexity Sensing)，不同类型的APP的不确定性不一样，优先调度不确定性最高的

## Prpbabilistic demand modeling

- `SLO schedule, agent, serve` **Efficient Serving of LLM Applications with Probabilistic Demand Modeling.** **Arxiv:2506**
  - SJTU & HUAWEI CLOUD
  - keywords: Probabilistic Demand Graph; LLM app serving; optimize e2e latency；
  - 问题：
    - 队列调度效率低下，因为不知道任务的执行时间只能当做黑盒
    - DAG动态性，后端容器预热延迟问题
  - insight：
    - 模块输入输出以及并行度的稳定性，高斯分布
    - 仅仅利用离线测量的均值不够，输入输出与之前的组件有关
  - 核心思想：建模DAG图，预测每个组件的资源需求，以及其后续依赖的概率关系，优化请求调度顺序以及容器预热判断
  - method
    - DAG建模：offline记录多个原始值，使用蒙特卡洛生成组件资源预计消耗；三种跨单元需求相关性模式，皮尔森相关分析，Online时当单元执行完成后，PDGraph进行条件预测优化，调整下游需求预测估计；
    - PDGraph-based Queue Management：
  - brainstorm：
    - 端侧多模型切换的问题
    - 组件利用频率的问题；预热有效性概率

## LLMSched

- `agent schedule, serve` **LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications**. **arXiv:2504**
  - SJTU
  - background
    - schedule Challenge：DAG uncertainty-topology; node exec duration
    - SFJ is useful to reduce ave latecy, but topology uncertainty makes wrong decision.
    - compound type
      - Predefined applications, fixed (rag)
      - chain like (react, iteration)
      - plan app (task-automation)
  - key insight: scheduling stages that reduce uncertainty, can obtain valuable information once they’re completed.
  - design
    - DAG define
      - Regular Stage, llm stage, dynamic stage
    - BN-based Profiler
      - offline, to identify stage reduces uncertainty
      - motivation: heatmaps show relation between stages, leverage inter-stage correlations
    - Entropy-based Uncertainty Quantification
      - to calculate which stage to schedule first
      - Uncertainty-aware Scheduler
      - **ε-greedy to mix schedule of SRFT and uncertainty schedule**
  - brainstorm
    - **only consider schedule layer, lacks fine-grained batch policy**

## DroidSpeak

- `kv cache, serving, multi-llm` **DroidSpeak: KV Cache Sharing for Cross-LLM Communicationand Multi-LLM Serving**. Yuhan Liu et.al. **NSDI'26**, [link](https://www.kdocs.cn/l/cs8n7GPZyQ2p)
- Chicago U & MS
- research problem:不同llm模型(from相同的base模型)的prefix kvcache怎么重用
- key insight:直接重用精度损失大;不同层敏感程度不一样;只有少部分层对input敏感;
- key idea:大部分层重用，小部分重计算。
- core method:
  - 把分散的重计算层连在一起，分散重计算开销大
  - offline+online确定重计算层
  - pipeline优化通信
- TLDR:
  - 前一个agent的输出feed到下一个agent的输入，是这么个情况吗？
  - multi llm，场景感觉比较稀少

- Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC. arxiv:2506

典型工作：
ayo,parrot,autellix
KV-COMM,KV-FLOW,Token-cake,Continuum
droidspeak,

重用:cacheblend,kvlink

重要的问题：

- 动态变化的DAG图没有考虑？对于kvcache复用的影响
- 长序列kv-cache的优化，大量的offload,agent下的pd分离(p-d)？
  - 甚至可以做agent为力度的PD分离,比如8张GPU,4张做prefill,2张做agent A的decode,两张做agent B的decode,并且加上弹性变化
- kv cache复用，maybe多lora maybe mooncake like? agent特性相关
- droidspeak 复用到agent场景，同一大小模型同一系列不同微调版本可以复用,是不是已经做了？
- 要不就做ktransformer like的场景
- plan可以重用,能不能找一找类似的,但是比较偏算法

---

> RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. arXiv:2404

- PKU & ByteDance
- not open-source
- keyword: rag prefix cache, serving, multi-level cache system,optim prefill latency
- key insight: cache the prefix kv-cache of the retrieval **hot** doc to reduce prefill latency
- background
  - rag bottleneck lies in prefill step for long seq len rag
  - recurrence of identical _**hot**_ documents across multiple requests
- method
  - Greedy-Dual-Size-Frequency (PGDSF) **replacement policy** to minimize cache miss
    - tree based; cost aware;
  - cache-aware request reordering
    - priority queue; cached length / recompute length
  - dynamic speculative pipelining.
    - vector search may produce the final results early in the retrieval step
    - overlap the retrieval and generation steps
- brainstorm
  - not consider different rag config, not all situation bottleneck lies in prefill
  - 0.3M docs in vector database, top-1 or 2 selection, is a representative config?
  - may be consider agent workflow info? powerinfer? on device?

> HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation. ISCA ’25

- HUST & NUS
- not open-source
- keywords: PIM based rag system; HBM-based PIM + DIMM-based PIM;
